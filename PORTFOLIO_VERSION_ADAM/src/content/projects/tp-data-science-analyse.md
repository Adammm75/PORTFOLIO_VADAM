---
name: 'TP Data Science - Analyse Statistique'
description: "Travaux pratiques avancés en Data Science avec analyses statistiques, visualisations et modélisation prédictive sur datasets réels."
tags: ['Data Science']
image: '../../../public/static/TP_Data_science.png'
link: 'https://github.com/Adammm75/TP_1_DATA_SCIENCE_PYTHON_L3_INFO'
startDate: '2024-03-15'
endDate: '2024-06-20'
---

# TP Data Science - Analyse Statistique Avancée

## 📊 Vue d'ensemble

Série de travaux pratiques en Data Science couvrant l'analyse statistique approfondie, la visualisation de données complexes et la modélisation prédictive. Ces TP démontrent la maîtrise des outils et techniques essentiels en science des données, depuis l'initiation à Jupyter jusqu'aux applications métier complexes.

## 💻 Code Source & Ressources

### 📁 [Repository GitHub - TP 1 & 2](https://github.com/Adammm75/TP_1_DATA_SCIENCE_PYTHON_L3_INFO)
*Accédez aux notebooks Jupyter complets des TP 1 et 2, incluant le code source, les datasets utilisés et les analyses détaillées.*

### 📁 [Repository GitHub - TP 4](https://github.com/Adammm75/TP_4_DATA_SCIENCE_PYTHON_L3_INFO)
*Code source du TP 4 sur la modélisation prédictive appliquée (cas AllLife Bank), incluant les notebooks d'analyse et de classification.*

---

## 📚 Table des Matières

### **📘 TP1 - Introduction à Jupyter et manipulation de base**
- Découverte environnement Jupyter Notebook
- Cellules Markdown vs Code Python
- Exécution pas-à-pas et documentation intégrée
- Premiers graphiques et principe de reproductibilité

### **📗 TP2 - Corrélation et régression linéaire**
- Relations entre variables et modélisation
- Méthodes de corrélation et régression linéaire
- Évaluation statistique des modèles
- Application aux données marketing (advertising.csv)

### **📙 TP4 - Modélisation prédictive appliquée (AllLife Bank)**
- Classification supervisée pour prédiction client
- Variables déterminantes et prétraitement
- Algorithmes ML et validation croisée
- Application métier banking et marketing

### **📊 Analyses Avancées Complémentaires**
- Tests statistiques et validation
- Visualisations interactives
- Techniques de preprocessing
- Applications sectorielles

## 📥 Télécharger les Notebooks

Consultez directement les notebooks Jupyter de chaque travail pratique :

<div style="display: flex; flex-wrap: wrap; gap: 1rem; margin: 2rem 0;">
  <a href="/static/TP1_lerebours_Mekkiou_Adam_STS.ipynb" download="TP1_lerebours_Mekkiou_Adam_STS.ipynb" style="display: inline-flex; align-items: center; gap: 0.5rem; background: linear-gradient(135deg, #3b82f6, #1d4ed8); color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; transition: all 0.3s; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
      <polyline points="7,10 12,15 17,10"></polyline>
      <line x1="12" y1="15" x2="12" y2="3"></line>
    </svg>
    📘 TP1 - Introduction Jupyter
  </a>

  <a href="/static/TP2_Mekkiou_Adam_STS1.ipynb" download="TP2_Mekkiou_Adam_STS1.ipynb" style="display: inline-flex; align-items: center; gap: 0.5rem; background: linear-gradient(135deg, #10b981, #059669); color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; transition: all 0.3s; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
      <polyline points="7,10 12,15 17,10"></polyline>
      <line x1="12" y1="15" x2="12" y2="3"></line>
    </svg>
    📗 TP2 - Corrélation & Régression
  </a>

  <a href="/static/TP4_Mekkiou_Adam_STS_LeRebours .ipynb" download="TP4_Mekkiou_Adam_STS_LeRebours.ipynb" style="display: inline-flex; align-items: center; gap: 0.5rem; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; padding: 0.75rem 1.5rem; border-radius: 0.5rem; text-decoration: none; font-weight: 600; transition: all 0.3s; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
    <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <path d="M21 15v4a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-4"></path>
      <polyline points="7,10 12,15 17,10"></polyline>
      <line x1="12" y1="15" x2="12" y2="3"></line>
    </svg>
    📙 TP4 - Modélisation Prédictive
  </a>
</div>

<div style="background: linear-gradient(135deg, #f3f4f6, #e5e7eb); border-left: 4px solid #3b82f6; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
  <p style="margin: 0; color: #374151; font-size: 0.9rem;">
    <strong>💡 Note :</strong> Ces notebooks Jupyter contiennent le code Python complet, les analyses statistiques, et les visualisations de chaque travail pratique. Ils peuvent être ouverts avec Jupyter Notebook, JupyterLab, ou Google Colab.
  </p>
</div>

## 🎯 Objectifs Pédagogiques

### Analyse Exploratoire
- **Exploration de datasets** multivariés complexes
- **Détection d'outliers** et traitement des valeurs aberrantes
- **Analyse de corrélations** et relations entre variables
- **Tests statistiques** : normalité, homoscédasticité, indépendance

### Visualisation Avancée
- **Graphiques multidimensionnels** avec matplotlib/seaborn
- **Visualisations interactives** avec Plotly
- **Heatmaps** et matrices de corrélation
- **Distributions** et analyses de densité

### Modélisation Statistique
- **Régression linéaire** et polynomiale
- **Tests d'hypothèses** et intervalles de confiance
- **ANOVA** et comparaisons de groupes
- **Analyse de variance** et facteurs explicatifs

## 📘 TP1 - Introduction à Jupyter et manipulation de base

### Objectifs
- **Découvrir l'environnement Jupyter Notebook** : Interface, navigation, gestion des fichiers
- **Comprendre la différence entre cellules Markdown** (texte explicatif, mise en forme) et **Code** (Python)
- **Se familiariser avec l'exécution pas-à-pas**, la documentation intégrée et les possibilités d'illustration (graphiques, images)

### Contenu
- **Prise en main technique** : lancement de Jupyter, structure des notebooks, exécution des cellules
- **Premières commandes Python** : variables, opérations arithmétiques, affichages simples (print), gestion de types
- **Visualisations initiales** : utilisation de bibliothèques standards pour afficher des graphiques simples
- **Introduction au principe de reproductibilité** des analyses (texte + code dans un même document)

### Conclusion
Ce TP a fourni les bases nécessaires pour utiliser Jupyter comme outil central de Data Science. L'étudiant a appris à combiner texte et code pour créer des rapports dynamiques, un savoir-faire fondamental pour la suite des travaux pratiques et tout projet de Data Science.

## 📗 TP2 - Corrélation et régression linéaire

### Objectifs
- **Comprendre la notion de relation** entre deux variables
- **Appliquer les méthodes de corrélation et de régression linéaire** pour modéliser ces relations
- **Évaluer la pertinence de la modélisation** par des mesures statistiques

### Contenu
- **Chargement des données** : fichier `advertising.csv` contenant des variables marketing (ex : budget pub et ventes)
- **Visualisation des relations** : nuages de points pour identifier les tendances
- **Corrélation et covariance** : calcul et interprétation des coefficients
- **Régression linéaire simple** :
  - Construction de la droite d'ajustement par la méthode des moindres carrés
  - Visualisation graphique et interprétation de la pente et de l'ordonnée à l'origine
- **Extension** : ajustement avec plusieurs variables pour comparer l'influence relative de chacune

### Résultats observés
- **Mise en évidence de relations proportionnelles** entre certaines dépenses et les ventes
- **Vérification que certaines variables influencent fortement** les résultats, tandis que d'autres n'ont que peu d'impact

### Conclusion
Ce TP a montré comment la corrélation et la régression linéaire permettent d'analyser et prédire des phénomènes. L'étudiant a appris à quantifier une relation, à l'illustrer graphiquement et à en tirer des conclusions interprétables dans un contexte métier (exemple : impact des investissements publicitaires sur les ventes).

## 📙 TP4 - Modélisation prédictive appliquée (cas AllLife Bank)

### Contexte
- **Entreprise** : AllLife Bank, en croissance, souhaite élargir sa base de clients emprunteurs
- **Problématique** : Identifier les clients passifs (dépôts) susceptibles de souscrire à un prêt personnel
- **Objectif métier** : Améliorer le ciblage des campagnes marketing pour augmenter le taux de conversion tout en fidélisant les clients existants

### Objectifs techniques
- **Construire un modèle de classification supervisée** pour prédire l'intérêt d'un client pour un prêt
- **Identifier les variables les plus déterminantes** dans la prise de décision
- **Évaluer la performance du modèle** avec des métriques adaptées

### Contenu
- **Exploration des données** : étude du profil des clients, identification des variables discriminantes (revenus, dépôts, ancienneté, etc.)
- **Prétraitement** : nettoyage des données, gestion des valeurs manquantes, transformation des variables catégorielles
- **Modélisation** :
  - Choix d'algorithmes de classification (par ex. Random Forest, Logistic Regression)
  - Entraînement et validation croisée pour limiter le sur-apprentissage
- **Évaluation** :
  - Utilisation de métriques telles que accuracy, précision, rappel, matrice de confusion, courbe ROC et AUC
  - Analyse des performances du modèle (prédictions correctes vs erreurs)
- **Interprétation métier** : variables influentes (âge, revenus, relation bancaire, historique d'emprunt)

### Résultats
- **Le modèle atteint de bonnes performances prédictives** (accuracy > 80%)
- **Identification des segments de clients** les plus susceptibles de souscrire à un prêt
- **Recommandations pour orienter les campagnes marketing** vers ces profils

### Conclusion
Ce TP illustre un projet complet de Data Science appliqué au marketing bancaire : de l'exploration à la modélisation. Il met en avant l'importance de la préparation des données et du choix des algorithmes. Au-delà de l'aspect technique, il démontre la valeur business que peut générer la Data Science pour améliorer la prise de décision et optimiser les stratégies de communication.

## 📈 Datasets Analysés Complémentaires

### 1. Dataset Immobilier
- **Variables** : Prix, surface, localisation, âge, équipements
- **Objectif** : Prédiction prix immobilier
- **Techniques** : Régression multiple, feature engineering
- **Résultats** : R² = 0.847, RMSE = 15.2K€

### 2. Dataset Marketing (advertising.csv)
- **Variables** : Budget TV, Radio, Journal, Ventes
- **Objectif** : Modélisation impact publicitaire
- **Techniques** : Corrélation, régression linéaire simple et multiple
- **Résultats** : Relations proportionnelles identifiées, variables influentes quantifiées

### 3. Dataset AllLife Bank
- **Variables** : Profil clients, revenus, dépôts, ancienneté, historique
- **Objectif** : Prédiction souscription prêt personnel
- **Techniques** : Classification supervisée, Random Forest, validation croisée
- **Résultats** : Accuracy > 80%, segmentation clients optimisée

## 🔬 Méthodologies Appliquées

### Préprocessing Avancé
```python
# Exemple de preprocessing
def advanced_preprocessing(df):
    # Détection outliers avec IQR
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))
    
    # Imputation valeurs manquantes
    df_cleaned = df.fillna(df.median())
    
    # Feature scaling
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df_cleaned)
    
    return df_scaled
```

### Tests Statistiques
- **Test de Shapiro-Wilk** : Vérification normalité (p-value > 0.05)
- **Test de Levene** : Homogénéité des variances
- **Test de Pearson** : Corrélations significatives (p < 0.05)
- **Test de Student** : Comparaisons de moyennes

### Validation Croisée
- **K-fold cross-validation** (k=5) pour validation robuste
- **Train/Test split** : 80%/20% avec stratification
- **Bootstrap sampling** : 1000 échantillons pour intervalles de confiance
- **Métriques multiples** : MAE, RMSE, R², AIC, BIC

## 📊 Visualisations Réalisées

### Graphiques Exploratoires
- **Histogrammes** avec courbes de densité
- **Boxplots** pour détection outliers
- **Scatter plots** avec droites de régression
- **Pairplots** pour relations multivariées

### Analyses Avancées
- **Heatmaps de corrélation** avec clustering hiérarchique
- **Graphiques de résidus** pour validation modèles
- **Courbes ROC** et métriques de classification
- **Graphiques de distribution** comparatives

### Visualisations Interactives
```python
import plotly.express as px
import plotly.graph_objects as go

# Exemple visualisation interactive
fig = px.scatter_3d(df, x='surface', y='prix', z='age',
                   color='quartier', size='nb_pieces',
                   title='Analyse 3D Prix Immobilier')
fig.show()
```

## 🧮 Techniques Statistiques Avancées

### Régression Multiple
- **Sélection de variables** : Stepwise, LASSO, Ridge
- **Interactions** : Tests d'interactions entre variables
- **Polynomial features** : Termes quadratiques et cubiques
- **Validation modèles** : AIC, BIC, adjusted R²

### Analyse de Variance
- **ANOVA one-way** : Comparaison groupes multiples
- **ANOVA two-way** : Effets principaux et interactions
- **ANCOVA** : Variables continue et catégorielle
- **Post-hoc tests** : Tukey HSD, Bonferroni

### Tests Non-Paramétriques
- **Mann-Whitney U** : Comparaison 2 groupes indépendants
- **Kruskal-Wallis** : Alternative non-paramétrique ANOVA
- **Spearman** : Corrélations monotones
- **Chi-carré** : Indépendance variables catégorielles

## 📈 Résultats et Insights

### Performance Modèles
| Modèle | Dataset | Métrique | Score |
|--------|---------|----------|-------|
| **Linear Regression** | Immobilier | R² | 0.847 |
| **Logistic Regression** | Santé | Accuracy | 91.3% |
| **Random Forest** | Marketing | F1-score | 0.876 |
| **SVM** | Immobilier | RMSE | 12.8K€ |

### Insights Métier
- **Immobilier** : Surface et localisation expliquent 85% variance prix
- **Marketing** : Segmentation améliore ROI de 23% vs approche globale
- **Santé** : 5 facteurs prédisent 91% des risques cardiovasculaires

### Recommandations
- **Feature engineering** crucial pour performance modèles
- **Validation croisée** indispensable pour éviter overfitting
- **Interpretabilité** vs performance : équilibre à trouver
- **Domain knowledge** essentiel pour feature selection

## 🛠️ Outils et Technologies

### Environnement Python
- **Jupyter Notebooks** : Développement interactif et prototypage
- **Pandas** : Manipulation et analyse de données tabulaires
- **NumPy** : Calculs numériques et algèbre linéaire
- **SciPy** : Tests statistiques et optimisation

### Visualisation
- **Matplotlib** : Graphiques statiques publication-ready
- **Seaborn** : Visualisations statistiques élégantes
- **Plotly** : Graphiques interactifs et dashboards
- **Bokeh** : Visualisations web interactives

### Machine Learning
- **Scikit-learn** : Modèles ML et preprocessing
- **Statsmodels** : Modèles statistiques et tests
- **XGBoost** : Gradient boosting performance
- **Feature-engine** : Feature engineering automatisé

## 📚 Compétences Développées

### Techniques
- **Analyse exploratoire** systématique et rigoureuse
- **Modélisation statistique** avec validation appropriée
- **Visualisation** claire et informative
- **Interpretation** statistique et métier des résultats

### Méthodologiques
- **Démarche scientifique** : hypothèses, tests, validation
- **Pensée critique** : remise en question des résultats
- **Communication** : présentation insights aux non-experts
- **Reproductibilité** : code documenté et versionné

## 🎯 Applications Pratiques

### Secteur Immobilier
- **Estimation automatique** prix biens immobiliers
- **Identification** facteurs de valorisation
- **Optimisation** stratégies pricing agences

### Marketing Digital
- **Segmentation clientèle** data-driven
- **Optimisation** allocation budgets publicitaires
- **Personnalisation** campagnes marketing

### Santé Publique
- **Screening** automatisé populations à risque
- **Prévention** ciblée facteurs modifiables
- **Optimisation** ressources sanitaires

---

*Travaux pratiques démontrant une maîtrise complète des techniques de Data Science, de l'analyse exploratoire à la modélisation prédictive avec validation rigoureuse.*
